{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gql, Client\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsHTTPTransport\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RateLimiter, GridAPIError, handle_api_error\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import gql\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Union, Dict, List, Optional, Any\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "from utilities import RateLimiter, GridAPIError, handle_api_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCollector1:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.rate_limiter = RateLimiter()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Add this to get the absolute path to your project root\n",
    "        self.base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "        \n",
    "        # Initialize clients\n",
    "        self.central_client = self._create_client('https://api-op.grid.gg/central-data/graphql')\n",
    "        self.stats_client = self._create_client('https://api-op.grid.gg/statistics-feed/graphql')\n",
    "\n",
    "    # Private utility methods\n",
    "    def _create_client(self, url: str) -> Client:\n",
    "        \"\"\"Create a GraphQL client for a specific endpoint.\"\"\"\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=url,\n",
    "            headers={'x-api-key': self.api_key},\n",
    "            retries=3\n",
    "        )\n",
    "        return Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "    def _load_query(self, filename: str) -> str:\n",
    "        \"\"\"Load a GraphQL query from file.\"\"\"\n",
    "        query_path = os.path.join(self.base_dir, 'queries', f'{filename}.graphql')\n",
    "        self.logger.info(f\"Looking for query file at: {query_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(query_path, 'r') as f:\n",
    "                return f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Query file not found at: {query_path}\")\n",
    "\n",
    "    def _execute_query(self, query: str, variables: Dict[str, Any], client: Client) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single query with rate limiting.\"\"\"\n",
    "        max_retries = 3\n",
    "        retry_delay = 5  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Wait for rate limit\n",
    "                self.rate_limiter.wait()\n",
    "                \n",
    "                # Execute query\n",
    "                self.logger.debug(f\"Executing query (attempt {attempt + 1})\")\n",
    "                result = client.execute(gql(query), variable_values=variables)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                if 'ENHANCE_YOUR_CALM' in str(e) or 'rate limit' in str(e).lower():\n",
    "                    if attempt < max_retries - 1:\n",
    "                        wait_time = retry_delay * (attempt + 1)\n",
    "                        self.logger.warning(f\"Rate limit hit, waiting {wait_time} seconds before retry\")\n",
    "                        time.sleep(wait_time)\n",
    "                        continue\n",
    "                raise handle_api_error(e)\n",
    "\n",
    "    def _execute_paginated_query(self, query: str, client: Client, variables: Dict = None, limit: int = None) -> List[Dict]:\n",
    "        \"\"\"Execute a paginated query and return all nodes, stopping when limit is reached.\"\"\"\n",
    "        if variables is None:\n",
    "            variables = {}\n",
    "        \n",
    "        all_nodes = []\n",
    "        has_next_page = True\n",
    "        after = None\n",
    "        page_number = 1\n",
    "    \n",
    "        while has_next_page:\n",
    "            try:\n",
    "                # Update variables for pagination\n",
    "                current_vars = {**variables, 'first': 50, 'after': after}\n",
    "            \n",
    "                # Execute query with rate limiting\n",
    "                result = self._execute_query(query, current_vars, client)\n",
    "            \n",
    "                # Get the correct data key (first key in result)\n",
    "                data_key = next(key for key in result.keys() if key != '__typename')\n",
    "                data = result[data_key]\n",
    "            \n",
    "                # Extract edges and nodes\n",
    "                edges = data.get('edges', [])\n",
    "                for edge in edges:\n",
    "                    if 'node' in edge:\n",
    "                        all_nodes.append(edge['node'])\n",
    "                    \n",
    "                        # Check if limit is reached\n",
    "                        if limit is not None and len(all_nodes) >= limit:\n",
    "                            self.logger.info(f\"Limit of {limit} items reached.\")\n",
    "                            return all_nodes[:limit]\n",
    "            \n",
    "               # Update pagination info\n",
    "                page_info = data.get('pageInfo', {})\n",
    "                has_next_page = page_info.get('hasNextPage', False)\n",
    "                after = page_info.get('endCursor') if has_next_page else None\n",
    "            \n",
    "                # Log progress\n",
    "                self.logger.info(f\"Processed page {page_number} - Got {len(edges)} items\")\n",
    "                page_number += 1\n",
    "            \n",
    "                # Add a small delay between pages to help prevent rate limiting\n",
    "                if has_next_page:\n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during pagination on page {page_number}: {str(e)}\")\n",
    "                raise\n",
    "                \n",
    "        return all_nodes\n",
    "\n",
    "\n",
    "    def get_tournaments(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all tournaments.\"\"\"\n",
    "        query = self._load_query('tournaments')\n",
    "        \n",
    "        try:\n",
    "            # Get all tournament nodes\n",
    "            tournaments = self._execute_paginated_query(query, self.central_client)\n",
    "            \n",
    "            # Process tournaments into a list of dictionaries\n",
    "            processed_tournaments = []\n",
    "            for tournament in tournaments:\n",
    "                # Extract title information\n",
    "                titles = tournament.get('titles', [])\n",
    "                title_ids = [t.get('id') for t in titles]\n",
    "                title_names = [t.get('name') for t in titles]\n",
    "                \n",
    "                # Create tournament record\n",
    "                tournament_data = {\n",
    "                    'id': tournament.get('id'),\n",
    "                    'name': tournament.get('name'),\n",
    "                    'name_short': tournament.get('nameShortened'),\n",
    "                    'start_date': tournament.get('startDate'),\n",
    "                    'end_date': tournament.get('endDate'),\n",
    "                    'private': tournament.get('private', False),\n",
    "                    'title_ids': title_ids,\n",
    "                    'title_names': title_names\n",
    "                }\n",
    "                processed_tournaments.append(tournament_data)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(processed_tournaments)\n",
    "            \n",
    "            # Convert dates\n",
    "            for date_col in ['start_date', 'end_date']:\n",
    "                if date_col in df.columns:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                    \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error collecting tournament data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_matches(self, days: int = 7) -> pd.DataFrame:\n",
    "        \"\"\"Get recent matches based on the series data from the API\n",
    "\n",
    "        Args:\n",
    "            days (int): Number of days of match history to collect\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing match information\n",
    "        \"\"\"\n",
    "        query = self._load_query('matches')\n",
    "    \n",
    "        # Calculate date range and format in the exact format the API expects\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "        # Format dates exactly like the API example\n",
    "        formatted_start = start_date.strftime('%Y-%m-%dT%H:00:00+00:00')\n",
    "        formatted_end = end_date.strftime('%Y-%m-%dT%H:00:00+00:00')\n",
    "    \n",
    "        # Replace the date placeholders in the query\n",
    "        modified_query = query.replace(\n",
    "            '\"2024-10-30T14:00:00+00:00\"',\n",
    "            f'\"{formatted_start}\"'\n",
    "        ).replace(\n",
    "            '\"2024-11-06T14:00:00+00:00\"',\n",
    "            f'\"{formatted_end}\"'\n",
    "        )\n",
    "    \n",
    "        variables = {\n",
    "            'first': 50,\n",
    "            'after': None\n",
    "        }\n",
    "    \n",
    "        try:\n",
    "            matches = self._execute_paginated_query(modified_query, self.central_client, variables)\n",
    "            processed_matches = []\n",
    "        \n",
    "            for match in matches:\n",
    "                if len(match.get('teams', [])) >= 2:\n",
    "                    match_data = {\n",
    "                        'id': match['id'],\n",
    "                        'series_title': match.get('title', {}).get('nameShortened'),\n",
    "                        'tournament_name': match.get('tournament', {}).get('nameShortened'),\n",
    "                        'start_time': match['startTimeScheduled'],\n",
    "                        'format_name': match.get('format', {}).get('name'),\n",
    "                        'format_short': match.get('format', {}).get('nameShortened'),\n",
    "                    \n",
    "                        # Team 1 information\n",
    "                        'team1_id': match['teams'][0]['baseInfo'].get('id'),\n",
    "                        'team1_name': match['teams'][0]['baseInfo'].get('name'),\n",
    "                        'team1_score_advantage': match['teams'][0].get('scoreAdvantage'),\n",
    "                    \n",
    "                        # Team 2 information\n",
    "                        'team2_id': match['teams'][1]['baseInfo'].get('id'),\n",
    "                        'team2_name': match['teams'][1]['baseInfo'].get('name'),\n",
    "                        'team2_score_advantage': match['teams'][1].get('scoreAdvantage')\n",
    "                    }\n",
    "                    processed_matches.append(match_data)\n",
    "        \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(processed_matches)\n",
    "        \n",
    "            # Convert datetime columns\n",
    "            if 'start_time' in df.columns:\n",
    "                df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error collecting match data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_all_titles(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch all available titles and return as a DataFrame.\"\"\"\n",
    "        query = self._load_query('titles')\n",
    "        variables = {}  # No filter, fetch all titles\n",
    "        try:\n",
    "            result = self._execute_query(query, variables, self.central_client)\n",
    "            titles = result.get('titles', [])\n",
    "            # Process titles into a list of dictionaries\n",
    "            processed_titles = []\n",
    "            for title in titles:\n",
    "                title_data = {\n",
    "                    'id': title.get('id'),\n",
    "                    'name': title.get('name'),\n",
    "                    'name_shortened': title.get('nameShortened'),\n",
    "                    'private': title.get('private')\n",
    "                }\n",
    "                processed_titles.append(title_data)\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(processed_titles)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching titles: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def get_players(self, limit: int = None, title_id: str = '28') -> pd.DataFrame:\n",
    "        \"\"\"Get players with specified fields, optionally limiting the number of players fetched.\"\"\"\n",
    "        query = self._load_query('players')\n",
    "        variables = {}\n",
    "        if title_id:\n",
    "            variables['filter'] = {'titleId': title_id}\n",
    "\n",
    "        try:\n",
    "            # Get all player nodes, with optional limit\n",
    "            players = self._execute_paginated_query(query, self.central_client, variables, limit=limit)\n",
    "\n",
    "            # Process players into a list of dictionaries\n",
    "            processed_players = []\n",
    "            for player in players:\n",
    "                player_data = {\n",
    "                    'id': player.get('id'),\n",
    "                    'nickname': player.get('nickname'),\n",
    "                    'title': player.get('title', {}).get('name'),\n",
    "                    'team_id': player.get('team', {}).get('id') if player.get('team') else None,\n",
    "                    'team_name': player.get('team', {}).get('name') if player.get('team') else None,\n",
    "                    'private': player.get('private', False)\n",
    "                }\n",
    "                processed_players.append(player_data)\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(processed_players)\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error collecting player data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def get_team_statistics(\n",
    "        self,\n",
    "        team_id: str,\n",
    "        time_window: Optional[str] = \"LAST_3_MONTHS\",\n",
    "        tournament_ids: Optional[List[str]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive team statistics either by time window or tournament IDs.\n",
    "        \n",
    "        Args:\n",
    "            team_id (str): The ID of the team\n",
    "            time_window (str, optional): Time window for statistics (e.g., \"LAST_3_MONTHS\")\n",
    "            tournament_ids (List[str], optional): List of tournament IDs to filter by\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processed team statistics\n",
    "        \"\"\"\n",
    "        query = self._load_query('team_statistics')\n",
    "        \n",
    "        # Build filter based on provided parameters\n",
    "        filter_dict = {}\n",
    "        if tournament_ids:\n",
    "            filter_dict['tournamentIds'] = {'in': tournament_ids}\n",
    "        elif time_window:\n",
    "            filter_dict['timeWindow'] = time_window\n",
    "            \n",
    "        variables = {\n",
    "            'teamId': team_id,\n",
    "            'filter': filter_dict\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            result = self._execute_query(query, variables, self.stats_client)\n",
    "            \n",
    "            if not result or 'teamStatistics' not in result:\n",
    "                return {}\n",
    "                \n",
    "            stats = result['teamStatistics']\n",
    "            \n",
    "            # Process and structure the statistics\n",
    "            processed_stats = {\n",
    "                'team_id': team_id,\n",
    "                'series_count': stats['series']['count'],\n",
    "                'game_count': stats['game']['count'],\n",
    "                'kills': {\n",
    "                    'total': stats['series']['kills']['sum'],\n",
    "                    'avg_per_series': stats['series']['kills']['avg'],\n",
    "                    'max_in_series': stats['series']['kills']['max'],\n",
    "                    'min_in_series': stats['series']['kills']['min']\n",
    "                },\n",
    "                'wins': {\n",
    "                    'count': stats['game']['wins']['count'],\n",
    "                    'percentage': stats['game']['wins']['percentage'],\n",
    "                    'current_streak': stats['game']['wins']['streak']['current'],\n",
    "                    'max_streak': stats['game']['wins']['streak']['max']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Process segment statistics\n",
    "            if 'segment' in stats:\n",
    "                processed_stats['segments'] = []\n",
    "                for segment in stats['segment']:\n",
    "                    segment_data = {\n",
    "                        'type': segment['type'],\n",
    "                        'count': segment['count'],\n",
    "                        'deaths': {\n",
    "                            'total': segment['deaths']['sum'],\n",
    "                            'avg': segment['deaths']['avg'],\n",
    "                            'max': segment['deaths']['max'],\n",
    "                            'min': segment['deaths']['min']\n",
    "                        }\n",
    "                    }\n",
    "                    processed_stats['segments'].append(segment_data)\n",
    "            \n",
    "            return processed_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching team statistics for team {team_id}: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def get_player_statistics(\n",
    "        self,\n",
    "        player_id: str,\n",
    "        time_window: Optional[str] = \"LAST_3_MONTHS\",\n",
    "        tournament_ids: Optional[List[str]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive player statistics either by time window or tournament IDs.\n",
    "        \n",
    "        Args:\n",
    "            player_id (str): The ID of the player\n",
    "            time_window (str, optional): Time window for statistics\n",
    "            tournament_ids (List[str], optional): List of tournament IDs to filter by\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processed player statistics\n",
    "        \"\"\"\n",
    "        query = self._load_query('player_statistics')\n",
    "        \n",
    "        # Build filter based on provided parameters\n",
    "        filter_dict = {}\n",
    "        if tournament_ids:\n",
    "            filter_dict['tournamentIds'] = {'in': tournament_ids}\n",
    "        elif time_window:\n",
    "            filter_dict['timeWindow'] = time_window\n",
    "            \n",
    "        variables = {\n",
    "            'playerId': player_id,\n",
    "            'filter': filter_dict\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            result = self._execute_query(query, variables, self.stats_client)\n",
    "            \n",
    "            if not result or 'playerStatistics' not in result:\n",
    "                return {}\n",
    "                \n",
    "            stats = result['playerStatistics']\n",
    "            \n",
    "            # Process and structure the statistics\n",
    "            processed_stats = {\n",
    "                'player_id': player_id,\n",
    "                'series_count': stats['series']['count'],\n",
    "                'game_count': stats['game']['count'],\n",
    "                'performance': {\n",
    "                    'kills': {\n",
    "                        'total': stats['series']['kills']['sum'],\n",
    "                        'avg_per_series': stats['series']['kills']['avg'],\n",
    "                        'max_in_series': stats['series']['kills']['max'],\n",
    "                        'min_in_series': stats['series']['kills']['min']\n",
    "                    },\n",
    "                    'wins': {\n",
    "                        'count': stats['game']['wins']['count'],\n",
    "                        'percentage': stats['game']['wins']['percentage'],\n",
    "                        'current_streak': stats['game']['wins']['streak']['current'],\n",
    "                        'max_streak': stats['game']['wins']['streak']['max']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add segment statistics if available\n",
    "            if 'segment' in stats:\n",
    "                processed_stats['segments'] = []\n",
    "                for segment in stats['segment']:\n",
    "                    segment_data = {\n",
    "                        'type': segment['type'],\n",
    "                        'count': segment['count'],\n",
    "                        'deaths': {\n",
    "                            'total': segment['deaths']['sum'],\n",
    "                            'avg': segment['deaths']['avg'],\n",
    "                            'max': segment['deaths']['max'],\n",
    "                            'min': segment['deaths']['min']\n",
    "                        }\n",
    "                    }\n",
    "                    processed_stats['segments'].append(segment_data)\n",
    "            \n",
    "            return processed_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching player statistics for player {player_id}: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def get_bulk_team_statistics(self, team_ids: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get statistics for multiple teams and return as a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            team_ids (List[str]): List of team IDs to fetch statistics for\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing statistics for all teams\n",
    "        \"\"\"\n",
    "        all_stats = []\n",
    "        for team_id in team_ids:\n",
    "            stats = self.get_team_statistics(team_id)\n",
    "            if stats:\n",
    "                # Flatten the nested dictionary for DataFrame compatibility\n",
    "                flat_stats = {\n",
    "                    'team_id': stats['team_id'],\n",
    "                    'series_count': stats['series_count'],\n",
    "                    'game_count': stats['game_count'],\n",
    "                    'total_kills': stats['kills']['total'],\n",
    "                    'avg_kills_per_series': stats['kills']['avg_per_series'],\n",
    "                    'win_count': stats['wins']['count'],\n",
    "                    'win_percentage': stats['wins']['percentage'],\n",
    "                    'current_win_streak': stats['wins']['current_streak'],\n",
    "                    'max_win_streak': stats['wins']['max_streak']\n",
    "                }\n",
    "                all_stats.append(flat_stats)\n",
    "                \n",
    "        return pd.DataFrame(all_stats)\n",
    "\n",
    "    def get_bulk_player_statistics(self, player_ids: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get statistics for multiple players and return as a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            player_ids (List[str]): List of player IDs to fetch statistics for\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing statistics for all players\n",
    "        \"\"\"\n",
    "        all_stats = []\n",
    "        for player_id in player_ids:\n",
    "            stats = self.get_player_statistics(player_id)\n",
    "            if stats:\n",
    "                # Flatten the nested dictionary for DataFrame compatibility\n",
    "                flat_stats = {\n",
    "                    'player_id': stats['player_id'],\n",
    "                    'series_count': stats['series_count'],\n",
    "                    'game_count': stats['game_count'],\n",
    "                    'total_kills': stats['performance']['kills']['total'],\n",
    "                    'avg_kills_per_series': stats['performance']['kills']['avg_per_series'],\n",
    "                    'win_count': stats['performance']['wins']['count'],\n",
    "                    'win_percentage': stats['performance']['wins']['percentage'],\n",
    "                    'current_win_streak': stats['performance']['wins']['current_streak'],\n",
    "                    'max_win_streak': stats['performance']['wins']['max_streak']\n",
    "                }\n",
    "                all_stats.append(flat_stats)\n",
    "                \n",
    "        return pd.DataFrame(all_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
